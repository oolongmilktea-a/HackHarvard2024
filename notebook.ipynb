{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                            key  fare_amount  \\\n",
      "0         24238194    2015-05-07 19:52:06.0000003          7.5   \n",
      "1         27835199    2009-07-17 20:04:56.0000002          7.7   \n",
      "2         44984355   2009-08-24 21:45:00.00000061         12.9   \n",
      "3         25894730    2009-06-26 08:22:21.0000001          5.3   \n",
      "4         17610152  2014-08-28 17:47:00.000000188         16.0   \n",
      "...            ...                            ...          ...   \n",
      "199994     3189201  2014-01-31 14:42:00.000000181         12.0   \n",
      "199995    42598914   2012-10-28 10:49:00.00000053          3.0   \n",
      "199996    16382965    2014-03-14 01:09:00.0000008          7.5   \n",
      "199998    20259894    2015-05-20 14:56:25.0000004         14.5   \n",
      "199999    11951496   2010-05-15 04:08:00.00000076         14.1   \n",
      "\n",
      "                pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
      "0       2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
      "1       2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
      "2       2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
      "3       2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
      "4       2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
      "...                         ...               ...              ...   \n",
      "199994  2014-01-31 14:42:00 UTC        -73.983070        40.760770   \n",
      "199995  2012-10-28 10:49:00 UTC        -73.987042        40.739367   \n",
      "199996  2014-03-14 01:09:00 UTC        -73.984722        40.736837   \n",
      "199998  2015-05-20 14:56:25 UTC        -73.997124        40.725452   \n",
      "199999  2010-05-15 04:08:00 UTC        -73.984395        40.720077   \n",
      "\n",
      "        dropoff_longitude  dropoff_latitude  passenger_count  distance_km  \n",
      "0              -73.999512         40.723217                1     1.683323  \n",
      "1              -73.994710         40.750325                1     2.457590  \n",
      "2              -73.962565         40.772647                1     5.036377  \n",
      "3              -73.965316         40.803349                3     1.661683  \n",
      "4              -73.973082         40.761247                5     4.475450  \n",
      "...                   ...               ...              ...          ...  \n",
      "199994         -73.972972         40.754177                1     1.122878  \n",
      "199995         -73.986525         40.740297                1     0.112210  \n",
      "199996         -74.006672         40.739620                1     1.875050  \n",
      "199998         -73.983215         40.695415                1     3.539715  \n",
      "199999         -73.985508         40.768793                1     5.417783  \n",
      "\n",
      "[174540 rows x 10 columns]\n",
      "Average Distance (km): 20.85534982536126\n",
      "Average Price: 11.359955250000002\n",
      "        fare_amount  distance_km pickup_time\n",
      "0               7.5     1.683323    19:52:06\n",
      "1               7.7     2.457590    20:04:56\n",
      "2              12.9     5.036377    21:45:00\n",
      "3               5.3     1.661683    08:22:21\n",
      "4              16.0     4.475450    17:47:00\n",
      "...             ...          ...         ...\n",
      "199995          3.0     0.112210    10:49:00\n",
      "199996          7.5     1.875050    01:09:00\n",
      "199997         30.9    12.850319    00:42:00\n",
      "199998         14.5     3.539715    14:56:25\n",
      "199999         14.1     5.417783    04:08:00\n",
      "\n",
      "[200000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame (update the file path)\n",
    "uber_df = pd.read_csv(\"uber.csv\")\n",
    "\n",
    "# Function to calculate the Haversine distance between two points on the Earth\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Radius of Earth in kilometers\n",
    "    r = 6371\n",
    "    return c * r  # Distance in kilometers\n",
    "\n",
    "# Step 1: Calculate the distance between pickup and dropoff points\n",
    "uber_df['distance_km'] = haversine(\n",
    "    uber_df['pickup_latitude'], \n",
    "    uber_df['pickup_longitude'], \n",
    "    uber_df['dropoff_latitude'], \n",
    "    uber_df['dropoff_longitude']\n",
    ")\n",
    "\n",
    "# Function to remove outliers using the IQR method\n",
    "def remove_outliers(df, column):\n",
    "    # Calculate the Interquartile Range (IQR)\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter the DataFrame to remove outliers\n",
    "    df_no_outliers = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    \n",
    "    return df_no_outliers\n",
    "\n",
    "# Apply the function to remove outliers from both 'fare_amount' and 'distance_km'\n",
    "uber_df_cleaned_no_outliers = remove_outliers(uber_df, 'fare_amount')\n",
    "uber_df_cleaned_no_outliers = remove_outliers(uber_df_cleaned_no_outliers, 'distance_km')\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(uber_df_cleaned_no_outliers)\n",
    "# Step 2: Drop the unnamed first column and coordinate columns\n",
    "uber_df_cleaned = uber_df.drop(columns=['Unnamed: 0', 'key', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'passenger_count'])\n",
    "\n",
    "# Step 3: Modify the 'pickup_datetime' column to only display the time (remove date)\n",
    "uber_df_cleaned['pickup_time'] = pd.to_datetime(uber_df_cleaned['pickup_datetime']).dt.time\n",
    "\n",
    "# Drop the original 'pickup_datetime' column\n",
    "uber_df_cleaned = uber_df_cleaned.drop(columns=['pickup_datetime'])\n",
    "\n",
    "# Step 4: Calculate the average distance\n",
    "average_distance = uber_df_cleaned['distance_km'].mean()\n",
    "\n",
    "# Step 5: Calculate the average price (if 'price' column exists)\n",
    "if 'fare_amount' in uber_df_cleaned.columns:\n",
    "    average_price = uber_df_cleaned['fare_amount'].mean()\n",
    "else:\n",
    "    average_price = \"Price column not found in the dataset.\"\n",
    "\n",
    "# Print the averages\n",
    "print(f\"Average Distance (km): {average_distance}\")\n",
    "print(f\"Average Price: {average_price}\")\n",
    "print(uber_df_cleaned)  \n",
    "\n",
    "# Optionally, save the cleaned DataFrame to a new CSV file\n",
    "# uber_df_cleaned.to_csv('uber_cleaned_with_distance_and_time.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using various models to predict a linear combination of distance and the pickup time to predict the fare amount of the Uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# Assuming that uber_df_cleaned is already available from your preprocessing steps\n",
    "# Preprocessing: Extract hour from pickup_time and add it as a new feature\n",
    "uber_df_cleaned['pickup_hour'] = pd.to_datetime(uber_df_cleaned['pickup_time'], format='%H:%M:%S').dt.hour\n",
    "\n",
    "# Step 1: Prepare features (distance_km and pickup_hour) and target (fare_amount)\n",
    "X = uber_df_cleaned[['distance_km', 'pickup_hour']]\n",
    "y = uber_df_cleaned['fare_amount']\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train models\n",
    "\n",
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "y_pred_gb = gb_reg.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate models\n",
    "\n",
    "def evaluate_model(y_test, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate Linear Regression\n",
    "evaluate_model(y_test, y_pred_lin, \"Linear Regression\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# Evaluate Gradient Boosting\n",
    "evaluate_model(y_test, y_pred_gb, \"Gradient Boosting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
